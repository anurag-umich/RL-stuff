{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This section will contain some excercises to learn about environment, state actions in open \n",
    "#AI gym and some important operations \n",
    "# Will create this later as we have to just follow open AI documentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We define a toy grid to implement dynamic programming based methods to solve bellman optimiality equation. We \n",
    "#consider a 4*4 grid and reaching the top left cell and bottom right cell is the goal . For each move we get a reward \n",
    "#of -1 for all moves that lead to a cell other than the goal state . On the edges of the grid if an action throws the \n",
    "#agent out of the grid then the agent remains in the same cellas before \n",
    "\n",
    "# Description of the environment : \n",
    "# n_states gives us total number of discrete states in our environment (env.n_states) , \n",
    "# n_actions gives number of actions (env.n_actions) , \n",
    "# Prob_dict provides us with a (transition probability to next state , next state ,reward,indicator of the goal state\n",
    "#(true or false) ) tuple in the same order given a state action pair ( Prob_dict[s][a])\n",
    "# The transition probability matrix can be accessed by env.Prob_dict , a tuple for a particular state and action \n",
    "# can be obtained by Prob_dict[s][a] \n",
    "\n",
    "# Do not make any changes to this code \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "class Gridwalking(object):\n",
    "    \n",
    "    def __init__(self,shape):\n",
    "        if not isinstance(shape, list) or not len(shape) == 2:\n",
    "            raise ValueError('shape must be a list of length 2')\n",
    "            \n",
    "            \n",
    "        self.shape = shape\n",
    "\n",
    "        n_states = np.prod(shape)\n",
    "        n_actions = shape[0]\n",
    "\n",
    "        y_max = shape[0]\n",
    "        x_max = shape[1]\n",
    "\n",
    "        Prob_dict = {}\n",
    "        grid_matrix = np.arange(n_states).reshape(shape)\n",
    "        iterator = np.nditer(grid_matrix, flags=['multi_index'])\n",
    "\n",
    "        while not iterator.finished:\n",
    "            s = iterator.iterindex\n",
    "            y, x = iterator.multi_index\n",
    "\n",
    "            Prob_dict[s] = {a : [] for a in range(n_actions)}\n",
    "\n",
    "            done = lambda s: s == 0 or s == (n_states - 1)\n",
    "            reward = 0.0 if done(s) else -1.0\n",
    "\n",
    "            # We're stuck in a terminal state\n",
    "            if  done(s):\n",
    "                Prob_dict[s][UP] = [(1.0, s, reward, True)]\n",
    "                Prob_dict[s][RIGHT] = [(1.0, s, reward, True)]\n",
    "                Prob_dict[s][DOWN] = [(1.0, s, reward, True)]\n",
    "                Prob_dict[s][LEFT] = [(1.0, s, reward, True)]\n",
    "            # Not a terminal state\n",
    "            else:\n",
    "                up_state = s if y == 0 else s - x_max\n",
    "                right_state = s if x == (x_max - 1) else s + 1\n",
    "                down_state = s if y == (y_max - 1) else s + x_max\n",
    "                left_state = s if x == 0 else s - 1\n",
    "                Prob_dict[s][UP] = [(1.0, up_state, reward, done(up_state))]\n",
    "                Prob_dict[s][RIGHT] = [(1.0, right_state, reward, done(right_state))]\n",
    "                Prob_dict[s][DOWN] = [(1.0, down_state, reward, done(down_state))]\n",
    "                Prob_dict[s][LEFT] = [(1.0, left_state, reward, done(left_state))]\n",
    "\n",
    "            iterator.iternext()\n",
    "\n",
    "        # Initial state distribution is uniform\n",
    "        initial_state_dist = np.ones(n_states) / n_states\n",
    "\n",
    "        # We expose the model of the environment for educational purposes\n",
    "        # This should not be used in any model-free learning algorithm\n",
    "        self.Prob_dict = Prob_dict\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will implement value iteration algorithm as described in Sutton and barto book / notes\n",
    "\n",
    "# Task 1 : Initialize a gridwalking environment with a 4*4 grid , print number of states, actions\n",
    "\n",
    "env = Gridwalking([4,4])\n",
    "\n",
    "# Write a function for doing one step look ahead given a state s \n",
    "    \n",
    "def One_step_lookahead(s, V):\n",
    "# Define (n_actions x 1) array to store values for each possible action\n",
    "    action_vals = np.zeros(n_actions)\n",
    "    \n",
    "# Implement the lookahead function to compute action values for each given action and store them in the above array\n",
    "# The function returns an array with action values for state s\n",
    "    for actions in range(env.n_actions):\n",
    "        trans_prob , next_state, reward , done  =  env.Prob_dict[s][a] \n",
    "        action_vals[a] += trans_prob * (reward + discount_factor * V[next_state])\n",
    "    \n",
    "    return action_vals\n",
    "    \n",
    "\n",
    "            \n",
    "# Fill in the necessary parts of the code to make the value iteration function running. This is based on value iteration \n",
    "# algorithm from sutton and barto\n",
    "def Value_iteration(env, epsilon=0.00001, discount_factor=1.0):\n",
    "    \n",
    "    V =  np.zeros(env.n_states)\n",
    "    while theta > epsilon:    \n",
    "        for s in range(env.n_states):\n",
    "        # Get look ahead action values for the state\n",
    "            Q = One_step_lookahead(s, V)\n",
    "        # Select the maximum value function \n",
    "            max_Action_val = np.amax(Q)\n",
    "        \n",
    "        # Computer the maximum difference between the max action value and value for each state\n",
    "            theta = max(theta, np.abs(V[s]- max_Action_val))\n",
    "        #update the value for state s\n",
    "            V[s] = max_Action_val\n",
    "            \n",
    "         return V  # return array with optimal value functions of each state \n",
    "        \n",
    "        \n",
    "# Write a function for finding optimal policy from optimal value function. The V array has optimal values for each state \n",
    "# obtained from value iteration algorithm\n",
    "\n",
    "def find_optimal_policy(V):\n",
    "    # create a n_states x n_actions array to store policy\n",
    "    policy = np.zeros([env.n_states,env.n_actions])\n",
    "    # for each state find the optimal action and update the policy\n",
    "    for s in range(len(V)):\n",
    "        Q = One_step_lookahead(s, V)\n",
    "        optimal_action = np.argmax(Q)\n",
    "        policy[s,optimal_action] = 1\n",
    "        \n",
    "    return policy\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def policy_improvement(env, policy_eval =evaluate_policy, discount_factor=1.0):\n",
    "    \n",
    "    policy = np.ones([env.n_states, env.n_actions]) / env.n_actions\n",
    "                    \n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = evaluate_policy(policy, env, discount_factor)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.n_states):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = np.zeros(env.n_actions)\n",
    "            for a in range(env.n_actions):\n",
    "                for prob, next_state, reward, done in env.Prob_dict[s][a]:\n",
    "                    action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            policy[s] = np.eye(env.n_actions)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, V    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.1, 0, 0.0, False), (0.8, 0, 0.0, False), (0.1, 4, 0.0, False)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "New Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
      "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
      "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
      "    If you step into one of those holes, you'll fall into the freezing water.\n",
      "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
      "    you navigate across the lake and retrieve the disc.\n",
      "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
      "    The surface is described using a grid like the following\n",
      "\n",
      "        SFFF\n",
      "        FHFH\n",
      "        FFFH\n",
      "        HFFG\n",
      "\n",
      "    S : starting point, safe\n",
      "    F : frozen surface, safe\n",
      "    H : hole, fall to your doom\n",
      "    G : goal, where the frisbee is located\n",
      "\n",
      "    The episode ends when you reach the goal or fall in a hole.\n",
      "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from frozen_lake import FrozenLakeEnv\n",
    "env = FrozenLakeEnv()\n",
    "print(env.__doc__)\n",
    "#print(env.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Backup(s, V, discount_factor):\n",
    "# Define (n_actions x 1) array to store values for each possible action\n",
    "    action_vals = np.zeros(env.nA)\n",
    "    \n",
    "# Implement the lookahead function to compute action values for each given action and store them in the above array\n",
    "# The function returns an array with action values for state s\n",
    "    for a in range(env.nA):\n",
    "        for trans_prob , next_state, reward , done in env.P[s][a]:\n",
    "             action_vals[a] += trans_prob * (reward + discount_factor * V[next_state])\n",
    "    \n",
    "    return action_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill in the necessary parts of the code to make the value iteration function running. This is based on value iteration \n",
    "# algorithm from sutton and barto\n",
    "def Value_iteration(env, epsilon=0.0001, discount_factor=0.8):\n",
    "    theta = 0.9\n",
    "    V =  np.zeros(env.nS)\n",
    "    n_iter = 0\n",
    "    while theta > epsilon:    \n",
    "        for s in range(env.nS):\n",
    "            print(\"state is\" , s)\n",
    "        # Get look ahead action values for the state\n",
    "            Q = Backup(s, V,discount_factor)\n",
    "            print(Q)\n",
    "            \n",
    "        # Select the maximum value function \n",
    "            max_Action_val = np.amax(Q)\n",
    "            print (V[s],\"--\",max_Action_val)\n",
    "            print(np.abs(V[s]- max_Action_val))\n",
    "        # Computer the maximum difference between the max action value and value for each state\n",
    "            theta = max(theta, np.abs(V[s]- max_Action_val))\n",
    "        #update the value for state s\n",
    "            V[s] = max_Action_val\n",
    "            n_iter += 1\n",
    "            print(\"theta is\" ,theta)\n",
    "            \n",
    "    return (V,n_iter)  # return array with optimal value functions of each state \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Value_iteration(env, epsilon=0.0001, discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write a function for finding optimal policy from optimal value function. The V array has optimal values for each state \n",
    "# obtained from value iteration algorithm\n",
    "\n",
    "def find_optimal_policy(V):\n",
    "    # create a n_states x n_actions array to store policy\n",
    "    policy = np.zeros([env.nS,env.nA])\n",
    "    # for each state find the optimal action and update the policy\n",
    "    for s in range(len(V)):\n",
    "        Q = Backup(s, V)\n",
    "        if stochastic == True:\n",
    "            Prob = Q/float(sum(Q))\n",
    "            policy[s] = Prob\n",
    "        else:\n",
    "            optimal_action = np.argmax(Q)\n",
    "            policy[s,optimal_action] = 1\n",
    "        \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Value_iteration(env, epsilon=0.0001, discount_factor=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will implement Policy iteration algorithm as described in Sutton and barto book / notes\n",
    "\n",
    "\n",
    "env = FrozenLakeEnv()\n",
    "\n",
    "def evaluate_policy(policy, env, discount_factor=0.9, epsilon=0.00001):\n",
    "    V = np.zeros(env.nS)\n",
    "    theta = 2\n",
    "    while theta > epsilon:\n",
    "        theta = 0\n",
    "        # For each state, perform a \"full backup\"\n",
    "        for s in range(env.nS):\n",
    "            curr_value = 0\n",
    "            # Look at the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                # For each action, look at the possible next states...\n",
    "                for  trans_prob, next_state, reward, done in env.P[s][a]:\n",
    "                    # Calculate the expected value\n",
    "                    curr_value += action_prob * trans_prob * (reward + discount_factor * V[next_state])\n",
    "            # How much our value function changed (across any states)\n",
    "            theta = max(theta, np.abs(curr_value - V[s]))\n",
    "            V[s] = curr_value\n",
    "        # Stop evaluating once our value function change is below a threshold\n",
    "        \n",
    "            \n",
    "    return np.array(V)\n",
    "\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = evaluate_policy(random_policy, env)        \n",
    "\n",
    "# Evaluate policy for different discount factor and state your observations\n",
    "# Report the states with maximum and minimum values and provide interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0044593 ,  0.0042131 ,  0.01006041,  0.00411379,  0.0067141 ,\n",
       "        0.        ,  0.02633197,  0.        ,  0.01867277,  0.05760567,\n",
       "        0.10697105,  0.        ,  0.        ,  0.13038228,  0.39148958,  0.        ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = FrozenLakeEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval =evaluate_policy, discount_factor=1.0):\n",
    "    \n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "                    \n",
    "    \n",
    "    while True:\n",
    "        # Evaluate the current policy\n",
    "        V = evaluate_policy(policy, env, discount_factor)\n",
    "        \n",
    "        # Will be set to false if we make any changes to the policy\n",
    "        policy_stable = True\n",
    "        \n",
    "        # For each state...\n",
    "        for s in range(env.nS):\n",
    "            # The best action we would take under the currect policy\n",
    "            chosen_a = np.argmax(policy[s])\n",
    "            \n",
    "            # Find the best action by one-step lookahead\n",
    "            # Ties are resolved arbitarily\n",
    "            action_values = np.zeros(env.nA)\n",
    "            for a in range(env.nA):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "            best_a = np.argmax(action_values)\n",
    "            \n",
    "            # Greedily update the policy\n",
    "            if chosen_a != best_a:\n",
    "                policy_stable = False\n",
    "            if stochastic == True:\n",
    "                policy[s] = action_values/float(sum(action_values))\n",
    "            else:\n",
    "                policy[s] = np.eye(env.nA)[best_a]\n",
    "        \n",
    "        # If the policy is stable we've found an optimal policy. Return it\n",
    "        if policy_stable:\n",
    "            return policy, V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  1.],\n",
       "        [ 0.,  0.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  1.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 1.,  0.,  0.,  0.]]),\n",
       " array([ 0.99586985,  0.99561945,  0.99544472,  0.99535509,  0.99590947,\n",
       "         0.        ,  0.79635578,  0.        ,  0.99600651,  0.99678958,\n",
       "         0.97703505,  0.        ,  0.        ,  0.99935714,  0.9996782 ,  0.        ]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_improvement(env, policy_eval =evaluate_policy, discount_factor=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
